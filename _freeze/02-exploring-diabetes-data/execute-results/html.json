{
  "hash": "c19018a5885c030b0899d5bffc80523a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"From Basics to Advanced Health Analytics: Exploring Diabetes Data\"\nsubtitle: \"An Exploratory Data Analysis (EDA) Tutorial\"\nauthor: \"Rafaela Ribeiro Lucas, Federica Gazzelloni, and Lucy Michaels\"\ndate: \"17 December, 2025\"\ndescription: |\n  This tutorial demonstrates exploratory data analysis (EDA) techniques on a simulated diabetes dataset for the years 2015 and 2025, covering data import, pre-processing, visualization, summary statistics, prevalence calculation, and statistical inference.\ncategories: [Tutorials, EDA, Diabetes, Clustering]\ntags: [Data Analysis, EDA, Diabetes, Descriptive Statistics]\nexecute:\n  echo: true\n  warning: false\n  message: false\nformat:\n  html:\n    code-copy: true\n    toc: true\n    toc-location: right\n---\n\n## Overview\n\nIn this session, we perform an exploratory data analysis on a simulated diabetes dataset for 2015 and 2025. The dataset used in this tutorial is **simulated**, this means that the 2015 and 2025 data are **not real patient records** and are **not drawn from GBD estimates or surveillance systems**. Instead, they consist of synthetic data designed to resemble realistic diabetes patterns over time, including changes in prevalence, laboratory measurements, and population structure.\n\nThe use of simulated data allows the analysis to focus on **methodology**, **workflow**, and **interpretation**, without privacy constraints or data access limitations. All results should therefore be interpreted as illustrative examples of analytical techniques rather than as epidemiological estimates.\n\nTo explore how diabetes prevalence and characteristics vary over time and across populations, in this tutorial we will cover:\n\n-   Data Import\n-   Pre-processing\n-   Exploratory Data Visualization (EDA)\n-   Summary Statistics\n-   Prevalence Calculation\n-   Qualitative Statistical Inference\n\n> Why Diabetes?\n\nDiabetes is a chronic health condition that affects how the body turns food into energy (blood glucose). It includes **Type 1**, **Type 2**, and **gestational diabetes**. The dataset used in this analysis contains information on individuals' `diabetes status`, `laboratory measurements` (such as HbA1c levels), `demographic information` (age, sex, country), and `other health-related variables`.\n\nAt the population level, diabetes is a major contributor to illness (`morbidity`) and premature death (`mortality`). Classified as a **non-communicable disease (NCD)**, it is associated with various complications, including `cardiovascular disease`, `kidney failure`, and `neuropathy.`\n\n### Research in Context\n\nThe **Global Burden of Disease (GBD)** study often identifies diabetes as a key driver of global health loss. According to **GBD 2023** estimates, approximately **561 million people** were living with diabetes worldwide in **2023**, which is roughly $7\\%$ of the world’s population (561 million out of $~8$ billion).[^1] In the same year, diabetes accounted for about **90.2 million disability-adjusted life years (DALYs)** globally, representing approximately $3.2\\%$ of total global DALYs. Diabetes also contributed substantially to non-fatal health loss, with an estimated **44.2 million years lived with disability (YLDs)** in 2023, representing about $4.5\\%$ of all global YLDs.[^2]\n\n[^1]: Global, regional, and national cascades of diabetes care, 2000–23: a systematic review and modelling analysis using findings from the Global Burden of Disease Study, Stafford, Lauryn K et al. The Lancet Diabetes & Endocrinology, Volume 13, Issue 11, 924 - 934 (<https://doi.org/10.1016/S2213-8587(25)00217-7>)\n\n[^2]: Burden of 375 diseases and injuries, risk-attributable burden of 88 risk factors, and healthy life expectancy in 204 countries and territories, including 660 subnational locations, 1990–2023: a systematic analysis for the Global Burden of Disease Study 2023 Hay, Simon I et al. The Lancet, Volume 406, Issue 10513, 1873 - 1922 (<https://doi.org/10.1016/S0140-6736(25)01637-X>)\n\nFrom an analytical perspective, diabetes is commonly studied using measures such as `prevalence rates,` `risk factors`, and `associations with other health conditions`. Typical statistical tools include `descriptive statistics`, `hypothesis testing` (e.g., chi-square tests for categorical variables) for group comparisons, and exploratory methods such as `clustering`, `regression analysis`, and, in some settings, `survival analysis`.\n\n### Research Questions\n\nTo focus the analysis, we begin by defining the research questions addressed in this tutorial:\n\n> 1.  Has the prevalence of diabetes changed significantly between 2015 and 2025?\n\n> 2.  Does the prevalence of diabetes differ significantly between countries in the year 2025?\n\n## Packages\n\nBefore we start, we load a small set of packages.\n\n### Install Required Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c(\"readxl\", \"janitor\", \"tidyverse\", \n                   \"scales\", \"crosstable\", \n                   \"vcd\", \"DescTools\",\n                   \"cluster\",\"clustMixType\"))\n```\n:::\n\n\n### Load Required Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Packages for Data Manipulation and Visualization\nlibrary(readxl) # For read_excel()\nlibrary(janitor) # For clean_names()\nlibrary(tidyverse) # For data manipulation and visualization\n# tidyverse::tidyverse_packages()\nlibrary(scales) # For scale_y_continuous(labels = scales::percent)\nlibrary(crosstable) # For crosstable()\n# Packages for Clustering\nlibrary(vcd)    # For assocstats()\nlibrary(DescTools) # For CramerV()\nlibrary(cluster) # For data manipulation\nlibrary(clustMixType) # For k-prototypes clustering\n```\n:::\n\n\n\n\n## Data Import\n\nData are stored in one Excel file with two sheets: one for 2015 and one for 2025. We read each sheet into R and clean the column names so they are consistent and easy to type.\n\nWe use the `read_excel` function from the `readxl` package to read the data and the `clean_names` function from the `janitor` package to clean the column names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- \"data/diabetes_study_filled_NEW.xlsx\"\n\nd2015 <- readxl::read_excel(path, \n                            sheet = \"2015\") \nd2025 <- readxl::read_excel(path, \n                            sheet = \"2025\") \n```\n:::\n\n\n## Pre-processing\n\nThe pre-processing phase is crucial for ensuring the quality and integrity of the data before conducting any analysis. Data are often messy and may contain **inconsistencies**, **missing values**, or **irrelevant information** that can affect the results of the analysis.\n\n### Data Manipulation\n\nCombine data for comparative analyses by year is straightforward, we *stack* the two datasets into a single table and add a year column. This is **data manipulation**; we merge the two datasets from 2015 and 2025 into a single dataset, adding a new column to indicate the year of each observation.\n\nIn particular, we use the `bind_rows` function from the `dplyr` package to stack the two datasets vertically, and the `mutate` function to create a new column called `year`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_raw <- bind_rows(\n  d2015 %>% mutate(year = 2015),\n  d2025 %>% mutate(year = 2025)) %>% \n  janitor::clean_names()\n```\n:::\n\n\n### Data Inspection\n\nThe first step is to check the initial structure of the data and identify any missing values. We can use the `head` function to view the first few rows of the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Checking initial structure\nhead(data_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   year country   age sex   bmi_cat       sdi     lab_hba1c diabetes_self_report\n  <dbl> <chr>   <dbl> <chr> <chr>         <chr>       <dbl> <chr>               \n1  2015 Brazil     42 woman Obesity       Interm…       4.9 no                  \n2  2015 Brazil     64 woman Normal weight Interm…       6.1 no                  \n3  2015 Brazil     82 woman Obesity       Interm…       4.8 no                  \n4  2015 Brazil     55 woman Normal weight Interm…       6   no                  \n5  2015 Brazil     59 woman Underweight   Interm…       5.6 no                  \n6  2015 Brazil     42 men   Normal weight Interm…       6.3 no                  \n# ℹ 1 more variable: gestational_diabetes <chr>\n```\n\n\n:::\n:::\n\n\nThen, we perform data inspection with the `str` or `glimpse` functions to understand the data types and structure of each variable. Both functions provide a concise summary of the dataset, including the number of observations, variables, and their respective data types.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(data_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 500\nColumns: 9\n$ year                 <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ country              <chr> \"Brazil\", \"Brazil\", \"Brazil\", \"Brazil\", \"Brazil\",…\n$ age                  <dbl> 42, 64, 82, 55, 59, 42, 54, 51, 73, 66, 54, 65, 4…\n$ sex                  <chr> \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"men…\n$ bmi_cat              <chr> \"Obesity\", \"Normal weight\", \"Obesity\", \"Normal we…\n$ sdi                  <chr> \"Intermediate\", \"Intermediate\", \"Intermediate\", \"…\n$ lab_hba1c            <dbl> 4.9, 6.1, 4.8, 6.0, 5.6, 6.3, 5.2, 5.8, 6.2, 6.0,…\n$ diabetes_self_report <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"…\n$ gestational_diabetes <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"…\n```\n\n\n:::\n:::\n\n\n### Handling Missing Values\n\nNext, we check for missing values (NA) with `is.na` function. This function returns a logical matrix indicating the presence of missing values in the dataset. To count the number of missing values in each column, we can use the `colSums` function in combination with `is.na`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Checking missing values (NA)\ncolSums(is.na(data_raw))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                year              country                  age \n                   0                    0                    0 \n                 sex              bmi_cat                  sdi \n                   0                    0                    0 \n           lab_hba1c diabetes_self_report gestational_diabetes \n                   0                    0                    0 \n```\n\n\n:::\n:::\n\n\nThere aren't missing values in the dataset, but if there were, we could handle them using various strategies such as `imputation`, `removal`, or `flagging`, depending on the nature and extent of the missing data.\n\n::: {.callout-note appearance=\"simple\" title=\"Question\"}\nHow would you handle missing values in a dataset? What strategies would you consider?\n:::\n\n### Creating Derived Variables\n\nData are usually not in the exact format needed for analysis. Therefore, we create new variables based on existing ones to facilitate the analysis. We are looking at making comparisons between two years to see how diabetes level changes along the time.\n\nIn this case, we create three derived variables:\n\n-   Laboratory-defined diabetes ($HbA1c ≥ 6.5$)\n-   Total diabetes (self-report OR laboratory, excluding gestational diabetes)\n-   Age groups (set of age bands to support grouped summaries)\n\nThis step involves using the `mutate` function from the `dplyr` package to create new columns based on conditions applied to existing columns. And we use the `if_else` and `case_when` functions to define the logic for these new variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_diabetes <- data_raw %>%\n  mutate(\n    # Laboratory-defined diabetes\n    diabetes_lab = if_else(lab_hba1c >= 6.5, \n                           \"yes\", \"no\", \n                           missing = NA_character_),\n    # Total diabetes (self-report OR laboratory, excluding gestational diabetes)\n    diabetes_total = case_when(\n      gestational_diabetes == \"yes\" ~ \"no\",\n      diabetes_self_report == \"yes\" | diabetes_lab == \"yes\" ~ \"yes\",\n      TRUE ~ \"no\"),\n    # Age groups\n    age_group = case_when(\n      age < 50 ~ \"40–49\",\n      age < 60 ~ \"50–59\",\n      TRUE ~ \"60+\")\n    )\n```\n:::\n\n\n## Exploratory data Analysis (EDA)\n\n`Exploratory Data Analysis (EDA)` is a crucial step in understanding the underlying patterns, relationships, and distributions within a dataset. It involves using various statistical and graphical techniques to summarize and visualize the data. EDA helps to identify potential issues, outliers, and trends that may inform subsequent analyses or modelling efforts.\n\n### Distribution plot for HbA1c (2015 only)\n\nLet's visualize the distribution of `HbA1c` levels for the year 2015 using a `histogram` combined with a `density plot`. This will help us understand the spread and central tendency of HbA1c values in that year.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = d2015, \n       aes(x = lab_hba1c)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 30,\n                 fill = \"lightblue\", \n                 color = \"grey70\") +\n  geom_density(color = \"darkblue\", \n               linewidth = 1) +\n  labs(title = \"HbA1c Distribution (2015)\",\n       x = \"HbA1c (%)\",\n       y = \"Density\") \n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nWe can see that the distribution of HbA1c levels in 2015 is right-skewed, with a peak around $5-6\\%$. This indicates that most individuals had HbA1c levels within the normal range, but there is a tail of higher values indicating some individuals with elevated HbA1c levels.\n\n### HbA1c distribution in both years\n\nTo compare the distribution of **HbA1c levels between 2015 and 2025**, we can create a `density plot` that overlays the distributions for both years. This will allow us to visually assess any changes in HbA1c levels over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_diabetes %>%\n  ggplot(aes(x = lab_hba1c, \n           fill = as.factor(year))) +\n  geom_density(alpha = 0.5,\n               linewidth = 0.2) +\n  labs(title = \"HbA1c Distribution — 2015 vs 2025\",\n       x = \"HbA1c (%)\",\n       y = \"Density\",\n       fill = \"Year\")\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Age distribution by year\n\nTo check the age distribution for both years, we can create `overlapping histograms`. This will help us visualize how the age distribution of the population has changed from 2015 to 2025.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = df_diabetes,\n       mapping = aes(x = age, \n                     fill = factor(year))) +\n  geom_histogram(position = \"identity\", \n                 color = \"grey80\",\n                 alpha = 0.5, bins = 30) +\n  labs(title = \"Age Distribution: 2015 vs 2025\",\n       fill = \"Year\",\n       x = \"Age\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n### Age Boxplots\n\nTo further explore the age distribution by `year` and gender (`sex`), we can create `boxplots`. Boxplots provide a visual summary of the distribution, median, quartiles, and potential outliers for age across different years and genders.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df_diabetes,\n       aes(x = factor(year), y = age, \n           fill = factor(year))) +\n  geom_boxplot(median.color = \"grey70\",) +\n  scale_fill_manual(values = c(\"2015\" = \"lightblue\", \n                             \"2025\" = \"salmon\")) +\n  facet_wrap(~sex) +\n  labs(title = \"Age Distribution by Year\",\n       x = \"Year\",\n       y = \"Age\",\n       fill = \"Year\") \n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\nThis plot shows the age distribution for both years separately for males and females. We can observe any shifts in median age or variability between the two years and across genders. We do not have outliers in this dataset.\n\n## Summary Statistics\n\nNow that we have explored the data visually, we can compute some summary statistics to quantify key characteristics of the dataset, and compare them across the two years.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_diabetes %>%\n  summarise(\n    n = n(),\n    age_mean = mean(age, na.rm = TRUE),\n    age_sd   = sd(age, na.rm = TRUE),\n    hba1c_mean = mean(lab_hba1c, na.rm = TRUE),\n    hba1c_sd   = sd(lab_hba1c, na.rm = TRUE)\n    ) %>%\n  round(2) %>% # Round to 2 decimal places\n  t() # Transpose for better layout\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\nn          500.00\nage_mean    56.34\nage_sd      11.86\nhba1c_mean   5.96\nhba1c_sd     0.92\n```\n\n\n:::\n:::\n\n\nThe results show the overall mean and standard deviation for age and HbA1c levels across the entire dataset, without stratifying by year.\n\nThe mean age is approximately 56 years, with a standard deviation of about 12 years, indicating a middle-aged population with some variability in age. The mean HbA1c level is around $6.2\\%$, with a standard deviation of about $1.5\\%$, suggesting that, on average, the population has HbA1c levels slightly above the normal range (typically $<5.7\\%$ for non-diabetic individuals), with considerable variability.\n\n### Comparisons by Year\n\nTo compare these statistics by year, we can use the `group_by` function to group the data by the `year` variable before summarising.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_diabetes %>%\n  group_by(year) %>%\n  summarise(\n    n = n(),\n    age_mean = mean(age, na.rm = TRUE),\n    hba1c_mean = mean(lab_hba1c, na.rm = TRUE)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n   year     n age_mean hba1c_mean\n  <dbl> <int>    <dbl>      <dbl>\n1  2015   250     56.7       5.87\n2  2025   250     56.0       6.05\n```\n\n\n:::\n:::\n\n\n## Prevalence Calculation\n\nTo calculate the prevalence of diabetes for each year, we can use the `group_by` and `summarise` functions to compute the mean of the binary indicator for diabetes status.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprev_diabetes <- df_diabetes %>%\n  group_by(year, diabetes_total) %>%\n  summarise(n = n(), .groups = \"drop\") %>%\n  group_by(year) %>% \n  mutate(prop = n / sum(n))\n\nprev_diabetes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n# Groups:   year [2]\n   year diabetes_total     n  prop\n  <dbl> <chr>          <int> <dbl>\n1  2015 no               212 0.848\n2  2015 yes               38 0.152\n3  2025 no               188 0.752\n4  2025 yes               62 0.248\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprev_summary <- df_diabetes %>%\n  group_by(year) %>%\n  summarise(prev_diabetes = mean(diabetes_total == \"yes\", \n                                 na.rm=TRUE))\nprev_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n   year prev_diabetes\n  <dbl>         <dbl>\n1  2015         0.152\n2  2025         0.248\n```\n\n\n:::\n:::\n\n\n### Prevalence Table with Confidence Intervals\n\nTo calculate prevalence with confidence intervals, we can use the `prop.test` function within a custom summarise operation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprev_ci <- df_diabetes %>%\n  group_by(year) %>%\n  summarise(\n    n = n(),\n    cases = sum(diabetes_total == \"yes\", na.rm = TRUE),\n    prev = cases / n,\n    ci_lower = prop.test(cases, n)$conf.int[1],\n    ci_upper = prop.test(cases, n)$conf.int[2]\n  )\nprev_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n   year     n cases  prev ci_lower ci_upper\n  <dbl> <int> <int> <dbl>    <dbl>    <dbl>\n1  2015   250    38 0.152    0.111    0.204\n2  2025   250    62 0.248    0.197    0.307\n```\n\n\n:::\n:::\n\n\n### Visualize diabetes prevalence by year\n\nTo visualize diabetes prevalence by year, we can create a bar plot with the `errorbars` representing the confidence intervals. This plot will show the proportion of individuals with diabetes for each year.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(prev_ci, \n       aes(x = factor(year), \n           y = prev, \n           fill = factor(year))) +\n  geom_col() +\n  geom_errorbar(aes(ymin = ci_lower, \n                    ymax = ci_upper), \n                width = 0.2) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"2015\" = \"lightblue\", \n                               \"2025\" = \"salmon\")) +\n  labs(title = \"Diabetes Prevalence by Year\",\n       x = \"Year\",\n       y = \"Prevalence (%)\",\n       fill = \"Year\")\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\nThis barplot shows the prevalence of diabetes for each year, along with the 95% confidence intervals. We can visually assess any changes in prevalence between 2015 and 2025.\n\n### Interaction\n\nTo explore the `interaction between gender, age, and diabetes status`, we can create a `faceted bar plot`. This plot will show the distribution of diabetes status across different age groups, separated by gender.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df_diabetes, \n       aes(x = age_group, \n           fill = diabetes_total)) +\n  geom_bar(position = \"fill\",\n           color = \"white\",\n           linewidth = 2) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"yes\" = \"lightblue\", \n                               \"no\" = \"salmon\")) +\n  labs(title = \"Diabetes Status by Age Group and Gender\",\n       x = \"Age Group\",\n       y = \"Proportion\",\n       fill = \"Diabetes Status\") +\n  facet_wrap(~sex)\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nInteraction between gender, age and diabetes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df_diabetes, \n       aes(x = diabetes_total, y = age, \n           fill = sex)) +\n  geom_boxplot(median.color = \"gray40\",\n               outlier.color = \"gray40\") +\n  scale_fill_manual(values = c(\"men\" = \"lightblue\",\n                               \"woman\" = \"salmon\")) +\n  labs(title = \"Age Distribution by Diabetes Status and Gender\",\n       x = \"Diabetes\",\n       y = \"Age\",\n       fill = \"Gender\") \n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\nHere, we can observe how the proportion of individuals with diabetes varies across age groups for both men and women. Additionally, the boxplot shows the age distribution for individuals with and without diabetes, separated by gender.\n\n\n## Qualitative Statistical Inference\n\nTo answer our research questions regarding diabetes prevalence, we will use the **Chi-Square Test of Independence**. This test is appropriate for categorical data and helps us determine whether there is a significant association between two categorical variables.\n\n> Question 1: The prevalence of diabetes changed significantly between 2015 and 2025?\n\n### Chi-square Test for Year and Diabetes Status\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable_year <- table(df_diabetes$year, \n                    df_diabetes$diabetes_total)\n\ntable_year\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      \n        no yes\n  2015 212  38\n  2025 188  62\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_year <- chisq.test(table_year)\n\nchi_year\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  table_year\nX-squared = 6.6125, df = 1, p-value = 0.01013\n```\n\n\n:::\n:::\n\n\nThe p-value is below 0.05, so we reject the null hypothesis and conclude that prevalence differs between 2015 and 2025 in this dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_year$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      \n        no yes\n  2015 200  50\n  2025 200  50\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardized Residuals\nres_year <- chi_year$stdres\nres_year\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      \n              no       yes\n  2015  2.683282 -2.683282\n  2025 -2.683282  2.683282\n```\n\n\n:::\n:::\n\n\n> Question 2: Will the prevalence of diabetes differ significantly between countries in the year 2025?\n\n### Chi-square Test for Country and Diabetes Status in 2025\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_diabetes_2025 <- df_diabetes %>% filter(year == 2025)\n```\n:::\n\n\nContingency table for countries and diabetes\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_2025 <- table(df_diabetes_2025$country,\n                  df_diabetes_2025$diabetes_total)\ntab_2025\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              \n               no yes\n  Brazil       42  25\n  China        39  11\n  Italy        40  10\n  South Africa 27   6\n  USA          40  10\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_2025 <- chisq.test(tab_2025)\nchi_2025\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab_2025\nX-squared = 7.8461, df = 4, p-value = 0.09738\n```\n\n\n:::\n:::\n\n\nThe p-value is greater than 0.05. Therefore, we fail to reject the null hypothesis and conclude that there is no statistically significant difference in diabetes prevalence between countries in 2025.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_2025$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              \n                   no    yes\n  Brazil       50.384 16.616\n  China        37.600 12.400\n  Italy        37.600 12.400\n  South Africa 24.816  8.184\n  USA          37.600 12.400\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardized Residuals\nresiduals <- chi_2025$stdres\nround(residuals, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              \n                  no   yes\n  Brazil       -2.77  2.77\n  China         0.51 -0.51\n  Italy         0.88 -0.88\n  South Africa  0.94 -0.94\n  USA           0.88 -0.88\n```\n\n\n:::\n:::\n\n\nIdentify where the greatest contribution lies:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmosaicplot(tab_2025,\n           main = \"Mosaic Plot — Diabetes by Country (2025)\",\n           color = TRUE, las = 1)\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-note appearance=\"simple\" title=\"Interpretation\"}\nInterpretation: We fail to reject the null hypothesis. There is no statistically significant evidence that diabetes prevalence differs between countries in 2025 at the 5% significance level.\n\nAlthough some countries show larger deviations from expected counts, these differences are not strong enough, overall, to conclude that prevalence differs significantly across countries.\n:::\n\n\n## K-prototypes Clustering\n\nIn this final analytical step, we explore whether individuals in the 2025 dataset can be grouped into distinct profiles based on a combination of clinical and demographic characteristics. Rather than focusing on hypothesis testing, this section introduces unsupervised learning as an exploratory tool to uncover structure in the data.\n\nClustering is particularly useful in public health settings when the goal is to:\n\n\t- identify subgroups with similar risk profiles,\n\t- explore heterogeneity within a population,\n\t- generate hypotheses for targeted interventions.\n\nBecause our dataset contains both numeric and categorical variables, we use the `k-prototypes algorithm`, which is specifically designed for mixed-type data.\n\n> Why k-prototypes?\n\nTraditional clustering methods such as `k-means` only work with numeric variables, while `k-modes` are limited to categorical data. The `k-prototypes` algorithm combines both approaches:\n\n\t- Euclidean distance is used for numeric variables,\n\t- matching dissimilarity is used for categorical variables,\n\t- a tuning parameter ($\\lambda$) balances the contribution of each type.\n\nThis makes k-prototypes well suited for health datasets that mix laboratory values (e.g. `HbA1c`) with demographic or clinical categories (e.g. `sex`, `BMI group`, `age band`).\n\n### Data Preparation for Clustering\n\nWe restrict the clustering analysis to observations from 2025, as the aim is to characterise the most recent population snapshot rather than temporal change.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(df_diabetes_2025)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [250 × 12] (S3: tbl_df/tbl/data.frame)\n $ year                : num [1:250] 2025 2025 2025 2025 2025 ...\n $ country             : chr [1:250] \"Brazil\" \"Brazil\" \"Brazil\" \"Brazil\" ...\n $ age                 : num [1:250] 44 83 45 48 48 47 55 66 57 66 ...\n $ sex                 : chr [1:250] \"woman\" \"men\" \"woman\" \"men\" ...\n $ bmi_cat             : chr [1:250] \"Normal weight\" \"Overweight\" \"Obesity\" \"Normal weight\" ...\n $ sdi                 : chr [1:250] \"Intermediate\" \"Intermediate\" \"Intermediate\" \"Intermediate\" ...\n $ lab_hba1c           : num [1:250] 6 6.1 5.8 5.9 5.3 6.3 8.8 5.4 5.3 6.4 ...\n $ diabetes_self_report: chr [1:250] \"no\" \"no\" \"no\" \"yes\" ...\n $ gestational_diabetes: chr [1:250] \"no\" \"no\" \"no\" \"no\" ...\n $ diabetes_lab        : chr [1:250] \"no\" \"no\" \"no\" \"no\" ...\n $ diabetes_total      : chr [1:250] \"no\" \"no\" \"no\" \"yes\" ...\n $ age_group           : chr [1:250] \"40–49\" \"60+\" \"40–49\" \"40–49\" ...\n```\n\n\n:::\n:::\n\n\nBefore clustering, variables must be correctly typed. Numeric variables should remain numeric, while categorical variables must be encoded as factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025 <- df_diabetes_2025 %>%\n  select(-year)%>%\n  mutate(across(where(is.character), as.factor))\n\nstr(cluster_data_2025)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [250 × 11] (S3: tbl_df/tbl/data.frame)\n $ country             : Factor w/ 5 levels \"Brazil\",\"China\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ age                 : num [1:250] 44 83 45 48 48 47 55 66 57 66 ...\n $ sex                 : Factor w/ 2 levels \"men\",\"woman\": 2 1 2 1 1 2 2 1 1 2 ...\n $ bmi_cat             : Factor w/ 4 levels \"Normal weight\",..: 1 3 2 1 2 2 2 3 2 2 ...\n $ sdi                 : Factor w/ 3 levels \"High\",\"Intermediate\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ lab_hba1c           : num [1:250] 6 6.1 5.8 5.9 5.3 6.3 8.8 5.4 5.3 6.4 ...\n $ diabetes_self_report: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 2 1 2 1 1 1 2 ...\n $ gestational_diabetes: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ diabetes_lab        : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 2 1 1 1 ...\n $ diabetes_total      : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 2 1 2 2 1 1 2 ...\n $ age_group           : Factor w/ 3 levels \"40–49\",\"50–59\",..: 1 3 1 1 1 1 2 3 2 3 ...\n```\n\n\n:::\n:::\n\n\nAt this stage, the dataset contains a mixture of numeric and categorical features suitable for k-prototypes clustering.\n\n### Checking Redundancy and Association Between Variables\n\nClustering can be distorted if highly redundant variables are included. We therefore assess association within numeric variables and dependence among categorical variables.\n\n#### Numeric Correlation (Pearson)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025 %>%\n  select_if(is.numeric) %>%\n  cor()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  age   lab_hba1c\nage       1.000000000 0.008397291\nlab_hba1c 0.008397291 1.000000000\n```\n\n\n:::\n:::\n\n\n`Age` and `HbA1c` show little correlation, so both can be retained. However, `age_group` is derived directly from age, so age is removed later to avoid duplicating the same information. So we will drop `age` from the clustering dataset.\n\n::: {.cell}\n\n```{.r .cell-code}\nfeatures_num <- c(#\"age\", \n                  \"lab_hba1c\")\n```\n:::\n\n\n\n#### Categorical Association (Cramér’s V)\n\nFor categorical variables, we use `Cramér’s V`, a normalized measure derived from the `chi-square` statistic:\n\n$$ \nV = \\sqrt{\\frac{\\chi^2 / n}{\\min(k - 1, r - 1)}} \n$$ {#eq-cramers_v}\n\nWhere:  \n-   ($\\chi^2$) is the Chi-square statistic  \n-   ($n$) is the total number of observations  \n-   ($k$) is the number of categories in one variable  \n-   ($r$) is the number of categories in the other variable\n\nCramer V is a measure of association between two nominal categorical variables. It ranges from 0 (no association) to 1 (perfect association). It is based on the Chi-square statistic and is useful for understanding the strength of association between categorical variables.\n\nThe inventor of Cramér's V, Harald Cramér, did not specify strict cut-offs for interpreting the values. However, in practice, researchers often use the following guidelines to interpret the strength of association:\n\n-   0 to 0.1: Negligible association\n-   0.1 to 0.3: Weak association\n-   0.3 to 0.5: Moderate association\n-   0.5 to 1.0: Strong association\n\nIn this case, we will identify pairs of categorical variables with **Cramér's V less than or equal to 0.5**, indicating weak to moderate associations.\n\nWe first examine the relationship between `country` and `sdi`, which are conceptually related using the `assocstats()` function from the `vcd` package:\n\n::: {.cell}\n\n```{.r .cell-code}\nvcd::assocstats(table(cluster_data_2025$country, cluster_data_2025$sdi))$cramer\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8998341\n```\n\n\n:::\n:::\n\nOr, we can even use the `CramerV()` function from the `DescTools` package:\n\n::: {.cell}\n\n```{.r .cell-code}\nDescTools::CramerV(cluster_data_2025$country, cluster_data_2025$sdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8998341\n```\n\n\n:::\n:::\n\n\nBoth packages show a value of 0.9, indicating a high association between `country` and `SDI.` The high association suggests that including both variables may overweight the same socioeconomic information. However, association with other categorical variables is weaker, so this decision is not automatic and requires judgement.\n\n**Test all pairs of categorical features**\n\nTo examine this systematically, we compute Cramér’s V for all pairs of categorical variables setting up a function to compute Cramér's V for any two categorical variables:\n\n::: {.cell}\n\n```{.r .cell-code}\ncramers_v <- function(x, y) CramerV(table(x, y))\n```\n:::\n\n\nExtract the names of all categorical features:\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025 %>%\n  select(where(is.factor)) %>%\n  names()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"country\"              \"sex\"                  \"bmi_cat\"             \n[4] \"sdi\"                  \"diabetes_self_report\" \"gestational_diabetes\"\n[7] \"diabetes_lab\"         \"diabetes_total\"       \"age_group\"           \n```\n\n\n:::\n:::\n\n\nWe focus on a subset of categorical features for clarity:\n\n::: {.cell}\n\n```{.r .cell-code}\nfeatures_cat <- c(\"sex\", \n                  \"bmi_cat\", \n                  #\"country\", \n                  #\"sdi\",\n                  \"age_group\")\n```\n:::\n\n\nThen we use the `expand_grid` function to create all possible pairs of categorical features, compute Cramér’s V for each pair, and filter for weak to moderate associations (Cramér's V <= 0.5):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpand_grid(feature1 = features_cat,\n            feature2 = features_cat) %>%\n  filter(feature1 < feature2) %>% # keep unique pairs only\n  rowwise() %>%\n  mutate(cramers_v = cramers_v(\n    cluster_data_2025[[feature1]],\n    cluster_data_2025[[feature2]])) %>%\n  # Filter for Cramér's V <= 0.5 to identify weak to moderate associations\n  filter(cramers_v <= 0.5) %>%\n  arrange(desc(cramers_v)) -> cramers_df\n\ncramers_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n# Rowwise: \n  feature1  feature2 cramers_v\n  <chr>     <chr>        <dbl>\n1 bmi_cat   sex         0.124 \n2 age_group sex         0.0698\n3 age_group bmi_cat     0.0578\n```\n\n\n:::\n:::\n\n\nThis confirms that the retained categorical variables are not strongly redundant.\n\n### Data Quality Checks: Outliers and Rare Categories\n\nBefore clustering, we check:\n\n\t- outliers in numeric variables,\n\t- imbalanced categories in categorical variables.\n\t\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025 %>%\n  select(lab_hba1c) %>%\n  ggplot() +\n  geom_boxplot(aes(x = lab_hba1c)) \n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\nWe can see the presence of outliers in `lab_hba1c` variable. To handle these outliers , we can perform a technique called **winsorization**, which involves capping extreme values to reduce their impact on the analysis. In this case, we will cap the values at the 1st and 99th percentiles.\n\nThe extreme values in `lab_hba1c` can disproportionately influence the clustering results. \n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(cluster_data_2025$lab_hba1c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.800   5.400   5.800   6.052   6.300   8.900 \n```\n\n\n:::\n:::\n\nThe maximum value of `lab_hba1c` before winsorization is 8.9.\n\nThen we calculate the upper whisker value using the interquartile range (IQR) method.\n\n::: {.cell}\n\n```{.r .cell-code}\nQ1 <- quantile(cluster_data_2025$lab_hba1c, 0.25)\nQ3 <- quantile(cluster_data_2025$lab_hba1c, 0.75)\nIQR_val <- Q3 - Q1\nupper_whisker <- Q3 + 1.5 * IQR_val\n```\n:::\n\n\nIn summary, winsorizing the `lab_hba1c` variable helps to mitigate the influence of extreme outliers on the clustering results.\n\n#### Winsorization of lab_hba1c\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the original values\ncluster_data_2025$lab_hba1c_original <- cluster_data_2025$lab_hba1c\n```\n:::\n\n\nWe use `pmin()` function to force the `lab_hba1c` outliers to equal the `upper whisker value`, it selects the minimum out of: the existing lab_hba1c value and the upper whisker value.\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025$lab_hba1c <- pmin(cluster_data_2025$lab_hba1c, upper_whisker)\n```\n:::\n\n\n\nAfter winsorization - where did the outliers go?\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(cluster_data_2025$lab_hba1c, \n        main = \"lab_hba1c – After Winsorization\",\n        horizontal = TRUE)\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\n#### Unbalanced categorical features\n\nWe check the distribution of categories in each categorical variable to ensure there are no extremely rare categories that could distort clustering.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_balance <- cluster_data_2025 %>%\n  select_if(is.factor) %>%\n  pivot_longer(cols = everything(),\n               names_to = \"variable\",\n               values_to = \"category\") %>%\n  count(variable, category, name = \"n\") %>%\n  group_by(variable) %>%\n  mutate(pct = round(100 * n / sum(n), 1),\n         is_rare = (n / sum(n)) < 0.05 ) %>%\n  ungroup() %>%\n  arrange(variable, desc(n))\n\ncat_balance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 5\n   variable  category          n   pct is_rare\n   <chr>     <fct>         <int> <dbl> <lgl>  \n 1 age_group 40–49            88  35.2 FALSE  \n 2 age_group 50–59            87  34.8 FALSE  \n 3 age_group 60+              75  30   FALSE  \n 4 bmi_cat   Normal weight   102  40.8 FALSE  \n 5 bmi_cat   Overweight       78  31.2 FALSE  \n 6 bmi_cat   Obesity          46  18.4 FALSE  \n 7 bmi_cat   Underweight      24   9.6 FALSE  \n 8 country   Brazil           67  26.8 FALSE  \n 9 country   China            50  20   FALSE  \n10 country   Italy            50  20   FALSE  \n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n\nNo categories are extremely rare (i.e., less than 5% of total), so we can retain all categorical variables for clustering.\n\n#### Scaling Numeric Variables and Final Variable Selection\n\nWhy scaling? Scaling numeric variables ensures that they contribute equally to the distance calculations used in clustering. Without scaling, variables with larger ranges can dominate the distance metric, leading to biased clustering results.\n\nWe then retain the final set of variables for clustering and standardise the numeric input.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025 <- cluster_data_2025 %>%\n  select(lab_hba1c,\n         sex,\n         age_group,\n         bmi_cat) %>%\n  # here is where scaling happens\n  mutate(lab_hba1c = as.numeric(scale(lab_hba1c)))\n```\n:::\n\n\n### Choosing the Number of Clusters\n\nWe use the elbow method, examining how within-cluster variation decreases as the number of clusters increases. What we do here is `run the k-prototypes algorithm` for a `range of cluster numbers (k = 2 to 10)` and record the total `within-cluster sum of squares (WSS)` for each k. We use the `map_dbl` function from the `purrr` package to map over the range of k values and store the WSS results. And the `kproto` function from the `clustMixType` package to perform k-prototypes clustering on the prepared dataset for 2025.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nkproto_wss_2025 <- purrr::map_dbl(2:10, function(k) {\n  \n  kpres <- kproto(cluster_data_2025,\n                  k, \n                  verbose = FALSE) \n    \n  kpres$tot.withinss\n})\n```\n:::\n\n\nAnd plot the WSS values to identify the \"elbow\" point where adding more clusters yields diminishing returns in reducing WSS.\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(2:10, kproto_wss_2025, type = \"b\",\n     xlab = \"Number of clusters (k)\",\n     ylab = \"Total within-cluster sum of squares\",\n     main = \"Elbow Method (2025)\")\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\nBased on the elbow plot, we choose `k = 5` as the optimal number of clusters.\n\n::: {.cell}\n\n```{.r .cell-code}\noptimal_k <- 5  # chosen from elbow\n```\n:::\n\n\n### Run K-prototypes Clustering\n\nWe use the `kproto` function again, but this time specifying the `optimal number of clusters (k = 5)` determined from the elbow method. We set a random seed for reproducibility, and configure the algorithm to run with 25 random starts and a maximum of 25 iterations per start to ensure convergence.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456789)\nkp_model <- kproto(cluster_data_2025, \n                   k = optimal_k,\n                   nstart = 25,\n                   iter.max = 25,\n                   lambda = NULL,\n                   verbose = FALSE)\n```\n:::\n\n\n\nAdd a cluster number (1 to 5) to each datapoint in the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025$cluster <- kp_model$cluster\n```\n:::\n\n\n#### Interpreting the Clusters\n\nKey diagnostic outputs help assess clustering quality and interpretability:\n\n::: {.cell}\n\n```{.r .cell-code}\nkp_model$size # Number of points in each cluster\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclusters\n 1  2  3  4  5 \n48 50 58 42 52 \n```\n\n\n:::\n:::\n\n\nLambda balances numerical (Euclidean) vs categorical (matching) distance:\n\n$$\n\\lambda =\\sum{\\frac{\\text{(std_dev of numerical features)}}{\\text{(n. numerical features)}}}\n$$ {#eq-lambda}\n\nCalculate lambda manually to verify:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkp_model$lambda # Lambda value used\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.614542\n```\n\n\n:::\n:::\n\n\nThe `withinss` values tell us how compact each cluster is. Lower is better, it means the patients in that cluster are more similar to each other.\n\n::: {.cell}\n\n```{.r .cell-code}\nkp_model$withinss\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  72.60207  58.85312  94.89224 110.92214  85.94492\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare cluster compactness\nbarplot(kp_model$withinss, \n        names.arg = paste(\"Cluster\", 1:5),\n        ylab = \"Within-cluster SS\",\n        main = \"Cluster Compactness (lower = more compact)\")\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\nThe total `within-cluster sum of squares` represents the overall compactness. When we did the elbow plot, we were looking at how this total changes as we add more clusters. We want tight clusters without over-fragmenting our data.\n\n::: {.cell}\n\n```{.r .cell-code}\nkp_model$tot.withinss\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 423.2145\n```\n\n\n:::\n:::\n\nThe cluster centres for categorical variables represent modal categories, while numeric centres reflect mean values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkp_model$centers\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    lab_hba1c   sex age_group       bmi_cat\n1 -0.71955321   men     50–59    Overweight\n2 -0.28653701 woman     50–59 Normal weight\n3 -0.51599890   men       60+ Normal weight\n4  1.84526774   men     40–49 Normal weight\n5  0.02484799 woman     40–49    Overweight\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025 %>%\n  group_by(cluster) %>%\n  summarise(\n    n = n(),\n    hba1c = round(mean(lab_hba1c), 1),\n    sex = names(which.max(table(sex))),\n    bmi_cat = names(which.max(table(bmi_cat))),\n    age = names(which.max(table(age_group)))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 6\n  cluster     n hba1c sex   bmi_cat       age  \n    <int> <int> <dbl> <chr> <chr>         <chr>\n1       1    48  -0.7 men   Overweight    50–59\n2       2    50  -0.3 woman Normal weight 50–59\n3       3    58  -0.5 men   Normal weight 60+  \n4       4    42   1.8 men   Normal weight 40–49\n5       5    52   0   woman Overweight    40–49\n```\n\n\n:::\n:::\n\n\n### External Validation Using Diabetes Status\n\nAlthough clustering is unsupervised, we can validate the result externally by checking whether clusters align with known diabetes status.\n\nAdd diabetes status back to cluster data:\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_data_2025$diabetes <- df_diabetes_2025$diabetes_total\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(table(cluster_data_2025$cluster, cluster_data_2025$diabetes))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(cluster_data_2025$cluster, cluster_data_2025$diabetes)\nX-squared = 138.5, df = 4, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nassocstats(table(cluster_data_2025$cluster, cluster_data_2025$diabetes))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    X^2 df P(> X^2)\nLikelihood Ratio 130.89  4        0\nPearson          138.50  4        0\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.597 \nCramer's V        : 0.744 \n```\n\n\n:::\n:::\n\n\nPractical implication for your clustering workflow:\n\nA Cramér’s V of 0.7443093 strongly suggests these two categorical variables carry overlapping information. If both are included in clustering, the algorithm may effectively *double count* the same underlying structure, increasing the weight of that dimension in cluster formation. \n\n### Cluster Quality: Silhouette Analysis\n\nSilhouette analysis measures how similar an object is to its own cluster compared to other clusters. The silhouette value ranges from -1 to 1, where a value close to 1 indicates that the object is well clustered, a value of 0 indicates that the object is on or very close to the decision boundary between two neighboring clusters, and negative values indicate that the object may have been assigned to the wrong cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_matrix <- cluster_data_2025 %>%\n  select(-cluster, -diabetes) %>%\n  cluster::daisy(metric = \"gower\")\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsil <- silhouette(kp_model$cluster, dist_matrix)\nmean(sil[, 3])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2102046\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sil, col = 1:optimal_k, \n     border = NA, \n     main = \"Silhouette Plot\")\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-67-1.png){width=672}\n:::\n:::\n\n\n\n### Key Clinical Insights\n\nThe k-prototypes clustering reveals clinically interpretable subgroups within the 2025 population, reflecting distinct metabolic profiles rather than arbitrary partitions.\n\n`Cluster 4` concentrates individuals with clearly `elevated HbA1c` levels (mean ≈ 8%), corresponding to the diagnosed diabetic population, although separation from neighbouring clusters is weaker than for some other groups.\n\n`Cluster 5` represents a pre-diabetic risk profile, characterised by younger individuals—predominantly women—who may benefit from early monitoring and preventive intervention.\n\n`Clusters 1–3` capture different non-diabetic phenotypes with broadly healthy metabolic profiles and varying demographic compositions.\n\n\nOverall, the clustering distinguishes diabetic from non-diabetic individuals in a meaningful way, as supported by external validation using diabetes status, while also highlighting substantial overlap consistent with diabetes risk existing along a continuum rather than as discrete categories.\n\nThis analysis reinforces that diabetes risk is not uniform across the population but varies systematically with age, sex, and BMI, supporting the use of stratified screening and targeted pre ,considering that this data is simulated and not real-world, the clinical interpretations should be viewed as illustrative rather than definitive.\n\n## Bonus\n\n### Frequency Table for BMI Categories\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrosstable(df_diabetes, by = \"year\", \n           cols = c(\"sex\",\"diabetes_total\",\n                    \"bmi_cat\",\"country\",\"sdi\")) %>%\n  as_flextable() \n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-cc071612{table-layout:auto;}.cl-cc01b942{font-family:'Helvetica';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cc01b956{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cc03d3e4{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-cc03d3e5{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-cc03e9e2{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(0, 0, 0, 1.00);border-top: 1.5pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cc03e9ec{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1.5pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cc03e9f6{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cc03e9f7{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cc03e9f8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cc03ea00{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cc03ea01{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cc03ea0a{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cc03ea0b{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-cc071612'><thead><tr style=\"overflow-wrap:break-word;\"><th  rowspan=\"2\"class=\"cl-cc03e9e2\"><p class=\"cl-cc03d3e4\"><span class=\"cl-cc01b942\">label</span></p></th><th  rowspan=\"2\"class=\"cl-cc03e9e2\"><p class=\"cl-cc03d3e4\"><span class=\"cl-cc01b942\">variable</span></p></th><th  colspan=\"2\"class=\"cl-cc03e9ec\"><p class=\"cl-cc03d3e4\"><span class=\"cl-cc01b942\">year</span></p></th></tr><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-cc03e9f6\"><p class=\"cl-cc03d3e4\"><span class=\"cl-cc01b942\">2015</span></p></th><th class=\"cl-cc03e9f6\"><p class=\"cl-cc03d3e4\"><span class=\"cl-cc01b942\">2025</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td  rowspan=\"2\"class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">sex</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">men</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">125 (50.00%)</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">125 (50.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">woman</span></p></td><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">125 (50.00%)</span></p></td><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">125 (50.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td  rowspan=\"2\"class=\"cl-cc03ea00\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">diabetes_total</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">no</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">212 (53.00%)</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">188 (47.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">yes</span></p></td><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">38 (38.00%)</span></p></td><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">62 (62.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td  rowspan=\"4\"class=\"cl-cc03ea00\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">bmi_cat</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">Normal weight</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">95 (48.22%)</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">102 (51.78%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">Obesity</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">56 (54.90%)</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">46 (45.10%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">Overweight</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">85 (52.15%)</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">78 (47.85%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">Underweight</span></p></td><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">14 (36.84%)</span></p></td><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">24 (63.16%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td  rowspan=\"5\"class=\"cl-cc03ea00\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">country</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">Brazil</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">33 (33.00%)</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">67 (67.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">China</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">66 (56.90%)</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">50 (43.10%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">Italy</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">34 (40.48%)</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">50 (59.52%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">South Africa</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">67 (67.00%)</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">33 (33.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">USA</span></p></td><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">50 (50.00%)</span></p></td><td class=\"cl-cc03e9f7\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">50 (50.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td  rowspan=\"3\"class=\"cl-cc03ea0a\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">sdi</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">High</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">100 (50.00%)</span></p></td><td class=\"cl-cc03ea01\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">100 (50.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">Intermediate</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">100 (50.00%)</span></p></td><td class=\"cl-cc03e9f8\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">100 (50.00%)</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cc03ea0b\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">Low</span></p></td><td class=\"cl-cc03ea0b\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">50 (50.00%)</span></p></td><td class=\"cl-cc03ea0b\"><p class=\"cl-cc03d3e5\"><span class=\"cl-cc01b956\">50 (50.00%)</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n### BMI Categories by Year\n\nThe distribution of BMI categories by year can be examined using counts and proportions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_diabetes %>%\n  count(year, bmi_cat) %>%\n  group_by(year) %>%\n  mutate(prop = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 4\n# Groups:   year [2]\n   year bmi_cat           n  prop\n  <dbl> <chr>         <int> <dbl>\n1  2015 Normal weight    95 0.38 \n2  2015 Obesity          56 0.224\n3  2015 Overweight       85 0.34 \n4  2015 Underweight      14 0.056\n5  2025 Normal weight   102 0.408\n6  2025 Obesity          46 0.184\n7  2025 Overweight       78 0.312\n8  2025 Underweight      24 0.096\n```\n\n\n:::\n:::\n\n\n### Visualizing BMI Categories by Year\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = df_diabetes, \n       aes(x = bmi_cat, fill = factor(year))) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"BMI categories by year\",\n       x = \"BMI category\",\n       y = \"Count\",\n       fill = \"Year\")\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-70-1.png){width=672}\n:::\n:::\n\n\n### Proportional Distribution of BMI Categories by Year\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df_diabetes, \n       aes(x = bmi_cat, fill = factor(year))) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"BMI categories by year (proportions)\",\n       x = \"BMI category\",\n       y = \"Proportion\",\n       fill = \"Year\")\n```\n\n::: {.cell-output-display}\n![](02-exploring-diabetes-data_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n\n\n",
    "supporting": [
      "02-exploring-diabetes-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/tabwid-1.1.3/tabwid.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/tabwid-1.1.3/tabwid.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}